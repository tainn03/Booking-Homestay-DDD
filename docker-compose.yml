version: "3.8"
networks:
  booking-net:
    driver: bridge
services:
  postgres:
    image: postgres:latest
    container_name: my-postgres
    restart: on-failure
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: sa2008
      POSTGRES_DB: booking
    ports:
      - "5432:5432"
    networks:
      - booking-net
    volumes:
      - postgresvolume:/var/lib/postgresql/data

  redis:
    image: redis:latest
    container_name: my-redis
    ports:
      - "6379:6379"
    environment:
      - REDIS_PASSWORD=
    command: [ "redis-server", "--protected-mode", "no" ]
    networks:
      - booking-net

  prometheus:
    image: prom/prometheus:latest
    container_name: my-prometheus
    restart: on-failure
    volumes:
      - ./environment/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./environment/data/prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    extra_hosts:
      - host.docker.internal:host-gateway
    ports:
      - "9090:9090"
    networks:
      - booking-net

  grafana:
    image: grafana/grafana
    container_name: my-grafana
    hostname: grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SING_UP=false
      - GF_SERVER_DOMAIN=localhost
      #enable logger
      - GF_LOG_MODE=console file
      - GF_LOG_FILTERS=alerting.notifier.slack:debug alermanager:debug ngalert:debug
    volumes:
      - ./environment/grafana-storage:/var/lib/grafana
    ports:
      - "3000:3000"
    networks:
      - booking-net

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.0
    container_name: my-elastic
    restart: on-failure
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - bootstrap.memory_lock=true
      - ELASTICSEARCH_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - "9200:9200"
    networks:
      - booking-net
    volumes:
      - elasticvolume:/usr/share/elasticsearch/data
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G

  #  logstash:
  #    image: docker.elastic.co/logstash/logstash:8.17.0
  #    container_name: my-logstash
  #    restart: on-failure
  #    build:
  #      context: ./environment/logstash/
  #      dockerfile: ./environment/logstash/Dockerfile
  #    ports:
  #      - "5000:5000"
  #      - "5033:5000/tcp"
  #      - "5022:5000/udp"
  #      - "9600:9600"
  #    environment:
  #      - LS_JAVA_OPTS=-Xms256m -Xmx256m
  #      - LOGSTASH_JDBC_USER=postgres
  #      - LOGSTASH_JDBC_PASSWORD=sa2008
  #      - LOGSTASH_JDBC_URL=jdbc:postgresql://postgres:5432/booking
  #      - CONFIG_SUPPORT_ESCAPE=true
  #      - ELASTICSEARCH_URL=http://elasticsearch:9200
  #    networks:
  #      - booking-net
  #    volumes:
  #      - logstashvolume:/usr/share/logstash/data
  #    depends_on:
  #      - elasticsearch
  #      - postgres

  kibana:
    image: docker.elastic.co/kibana/kibana:8.17.0
    container_name: my-kibana
    restart: on-failure
    ports:
      - "5601:5601"
    networks:
      - booking-net
    depends_on:
      - elasticsearch

    # Kafka visualization tool
  kafka-ui:
    container_name: my-kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - "8081:8080"  # Map Kafka UI to port 8080 on the host
    depends_on:
      kafka1:
        condition: service_healthy
    environment:
      KAFKA_CLUSTERS_0_NAME: huta-ddd      # Cluster name
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:9092  # Cluster bootstrap servers
      DYNAMIC_CONFIG_ENABLED: 'true'
    networks:
      - booking-net

    # Kafka cluster
  kafka1:
    image: 'bitnami/kafka:3.5'
    container_name: kafka1
    ports:
      - 9192:9092  # Broker port
      - 9193:9094    # Controller port
    environment:
      ### General configurations
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      # Plantext dành cho các kết nối nội bộ Docker, Controller lắng nghe các kết nối cho KRaft (Zookeeper), External dành cho các kết nối từ bên ngoài
      # Nếu spring boot kết nối từ docker host thì sử dụng PLAINTEXT://kafka1:9092, nếu kết nối từ bên ngoài thì sử dụng EXTERNAL://:9194
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      # Define security protocols
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      # External map cổng 9193 với 9094, 9092 là cổng kết nối nội bộ Docker
      - KAFKA_CFG_ADVERTISED_LISTENERS=EXTERNAL://127.0.0.1:9193,PLAINTEXT://kafka1:9092
      # Interbroker listener name
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      # Cluster ID for Kafka, must be the same across the cluster. Use a generated UUID
      - KAFKA_KRAFT_CLUSTER_ID=LelM2dIFQkiUFvXCEcqRWA
      # Cluster address
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka1:9093
      # Allow PLAINTEXT listener (default is false; not recommended for production)
      - ALLOW_PLAINTEXT_LISTENER=yes
      # Set maximum and initial memory for the broker
      - KAFKA_HEAP_OPTS=-Xmx512M -Xms256M
      # Enable auto-creation of topics
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      # Message retention period (in milliseconds), set to 7 days
      - KAFKA_LOG_RETENTION_MS=604800000
      ### Broker configurations
      # Define external access address (host IP and port) -> get Ip: run docker inspect kafka1
      # - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://${KAFKA1_IP}:9092
      # Broker ID, must be unique
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_NODE_ID=1
    volumes:
      - ./environment/kafka/data/bitnami/kafka1:/bitnami/kafka
    networks:
      - booking-net
    healthcheck:
      test: [ "CMD", "kafka-topics.sh", "--list", "--bootstrap-server", "localhost:9092" ]
      interval: 5s
      timeout: 10s
      retries: 10

volumes:
  postgresvolume:
  elasticvolume:
  logstashvolume:
